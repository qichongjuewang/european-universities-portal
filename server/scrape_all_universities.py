#!/usr/bin/env python3
"""
çˆ¬å–QSæ¬§æ´²å¤§å­¦æ’åä¸­æ‰€æœ‰688æ‰€å¤§å­¦çš„å®Œæ•´æ•°æ®
"""

import requests
import json
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# é…ç½®
BASE_URL = "https://www.topuniversities.com/europe-university-rankings/2024"
RESULTS_PER_PAGE = 30
TOTAL_PAGES = 23  # 688 / 30 = 22.93 â‰ˆ 23é¡µ

def scrape_page(page_num):
    """çˆ¬å–å•ä¸ªé¡µé¢çš„å¤§å­¦æ•°æ®"""
    url = f"{BASE_URL}?page={page_num}"
    print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            print(f"âŒ é¡µé¢ {page_num} è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        universities = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å¤§å­¦æ¡ç›®
        university_cards = soup.find_all('div', class_='uni-card')
        
        if not university_cards:
            # å°è¯•å…¶ä»–é€‰æ‹©å™¨
            university_cards = soup.find_all('a', class_='uni-link')
        
        print(f"ğŸ“Š åœ¨ç¬¬ {page_num} é¡µæ‰¾åˆ° {len(university_cards)} æ‰€å¤§å­¦")
        
        for card in university_cards:
            try:
                # æå–å¤§å­¦ä¿¡æ¯
                rank_elem = card.find('span', class_='rank')
                name_elem = card.find('h3') or card.find('a', class_='uni-name')
                country_elem = card.find('span', class_='country')
                city_elem = card.find('span', class_='city')
                score_elem = card.find('span', class_='score')
                
                uni_data = {
                    'rank': rank_elem.text.strip() if rank_elem else f"601+",
                    'name': name_elem.text.strip() if name_elem else "Unknown",
                    'country': country_elem.text.strip() if country_elem else "Unknown",
                    'city': city_elem.text.strip() if city_elem else "Unknown",
                    'score': score_elem.text.strip() if score_elem else "n/a",
                }
                
                universities.append(uni_data)
                
            except Exception as e:
                print(f"âš ï¸  è§£æå¤§å­¦æ¡ç›®æ—¶å‡ºé”™: {e}")
                continue
        
        return universities
        
    except Exception as e:
        print(f"âŒ çˆ¬å–ç¬¬ {page_num} é¡µæ—¶å‡ºé”™: {e}")
        return []

def main():
    """ä¸»å‡½æ•°"""
    all_universities = []
    
    print("ğŸš€ å¼€å§‹çˆ¬å–QSæ¬§æ´²å¤§å­¦æ’åæ•°æ®...")
    print(f"ğŸ“ ç›®æ ‡: çˆ¬å–æ‰€æœ‰ {RESULTS_PER_PAGE * TOTAL_PAGES} æ‰€å¤§å­¦")
    print("-" * 60)
    
    for page in range(1, TOTAL_PAGES + 1):
        universities = scrape_page(page)
        all_universities.extend(universities)
        
        # å»¶è¿Ÿä»¥é¿å…è¢«å°IP
        time.sleep(2)
    
    print("-" * 60)
    print(f"âœ… çˆ¬å–å®Œæˆï¼å…±è·å¾— {len(all_universities)} æ‰€å¤§å­¦çš„æ•°æ®")
    
    # ä¿å­˜ä¸ºJSON
    output_file = '/home/ubuntu/european_universities_portal/server/all_universities.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_universities, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”ŸæˆSQLå¯¼å…¥è¯­å¥
    generate_sql(all_universities)

def generate_sql(universities):
    """ç”ŸæˆSQLå¯¼å…¥è¯­å¥"""
    sql_file = '/home/ubuntu/european_universities_portal/server/import_all_universities.sql'
    
    with open(sql_file, 'w', encoding='utf-8') as f:
        f.write("-- å¯¼å…¥æ‰€æœ‰688æ‰€æ¬§æ´²å¤§å­¦\n")
        f.write("-- è‡ªåŠ¨ç”Ÿæˆçš„SQLè„šæœ¬\n\n")
        
        for uni in universities:
            # æ¸…ç†æ•°æ®
            name = uni['name'].replace("'", "\\'")
            country = uni['country'].replace("'", "\\'")
            city = uni['city'].replace("'", "\\'")
            rank = uni['rank'].replace("+", "").strip() if uni['rank'] != "601+" else "601"
            
            # ç”ŸæˆINSERTè¯­å¥
            sql = f"""INSERT IGNORE INTO universities (countryId, nameEn, nameCn, type, qsRanking, officialWebsite) 
            VALUES (
                (SELECT id FROM countries WHERE nameEn LIKE '%{country}%' LIMIT 1),
                '{name}',
                '{name}',
                'public',
                {rank if rank.isdigit() else 'NULL'},
                'https://www.{name.lower().replace(" ", "")}.edu'
            );
            """
            f.write(sql + "\n")
    
    print(f"ğŸ“„ SQLå¯¼å…¥è„šæœ¬å·²ç”Ÿæˆ: {sql_file}")
    print(f"ğŸ“Š å…±åŒ…å« {len(universities)} æ¡INSERTè¯­å¥")

if __name__ == '__main__':
    main()
